{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import pickle\n",
    "from PIL import Image\n",
    "from mtcnn import MTCNN\n",
    "from tensorflow import keras\n",
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /home/vitoria/anaconda3/envs/face_recognition/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nWARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
    }
   ],
   "source": [
    "facenet_model = keras.models.load_model('./facenet_keras.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = pickle.load(open('./mlp_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /home/vitoria/anaconda3/envs/face_recognition/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From /home/vitoria/anaconda3/envs/face_recognition/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From /home/vitoria/anaconda3/envs/face_recognition/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nWARNING:tensorflow:From /home/vitoria/anaconda3/envs/face_recognition/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n\nWARNING:tensorflow:From /home/vitoria/anaconda3/envs/face_recognition/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n\nWARNING:tensorflow:From /home/vitoria/anaconda3/envs/face_recognition/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n\nWARNING:tensorflow:From /home/vitoria/anaconda3/envs/face_recognition/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n\nWARNING:tensorflow:From /home/vitoria/anaconda3/envs/face_recognition/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nWARNING:tensorflow:From /home/vitoria/anaconda3/envs/face_recognition/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n\nWARNING:tensorflow:From /home/vitoria/anaconda3/envs/face_recognition/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n\n"
    }
   ],
   "source": [
    "detector = MTCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo a descrição e a cor dos labels para serem usados na OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_description = {\n",
    "    0: 'NO MASK',\n",
    "    1: 'MASK'\n",
    "}\n",
    "\n",
    "color = {\n",
    "    0: (0, 0, 255),\n",
    "    1: (0, 255, 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faces(image, size=(160, 160)):\n",
    "\n",
    "    '''\n",
    "      Esta função é responsável por extrair as faces de uma imagem através do modelo MTCNN.\n",
    "      Parâmetros:\n",
    "        * image: uma imagem de qualquer dimensão;\n",
    "        * size: refere-se ao shape de redimensionamento das faces, o padrão é (160, 160).\n",
    "      Retorno: uma lista de imagens (faces) do tipo Image Pillow. Caso nenhuma face tenha sido detectada, o retorno é uma lista vazia.\n",
    "    '''\n",
    "\n",
    "    # Transformando imagem em um array numpy\n",
    "    img = np.asarray(image)\n",
    "\n",
    "    # Capturando as faces da imagem através da MTCNN\n",
    "    results = detector.detect_faces(np.asarray(image))\n",
    "\n",
    "    # Lista para armazenar as faces\n",
    "    faces = []\n",
    "\n",
    "    # Percorrendo a lista de faces detectadas\n",
    "    for i in range(len(results)):\n",
    "      \n",
    "      try:\n",
    "\n",
    "        # Caso a face tenha sido detectada com mais de 95% de certeza, essa condição é verdadeira\n",
    "        if results[i]['confidence'] > 0.95:\n",
    "\n",
    "          # Extraindo os pontos da face\n",
    "          x1, y1, w, h = results[i]['box']\n",
    "          x2, y2 = x1 + w, y1 + h\n",
    "\n",
    "          # Extraindo a face da imagem fazendo slice nos pontos identificados pela MTCNN\n",
    "          face = image[y1:y2, x1:x2]\n",
    "\n",
    "          # Adicionando a face encontrada e suas informações na lista de faces\n",
    "          faces.append({\n",
    "              'x1': x1,\n",
    "              'y1': y1,\n",
    "              'x2': x2,\n",
    "              'y2': y2,\n",
    "              'face': np.array(Image.fromarray(face).resize(size)),\n",
    "              'confidence': results[i]['confidence']\n",
    "          })\n",
    "\n",
    "      except:\n",
    "        continue\n",
    "\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(face, facenet_model):\n",
    "\n",
    "  '''\n",
    "    Esta função é responsável por receber uma imagem e fornecer os embeddings da mesma, através do modelo FaceNet.\n",
    "    Parâmetros:\n",
    "      * face: é uma imagem com dimensão 160x160, referente a uma face extraída de uma imagem;\n",
    "      * facenet_model: é uma instância do modelo FaceNet previamente treinado, responsável por gerar os embeddings da imagem.\n",
    "    Retorno: um array numpy com 128 posições, referentes às 128 características da FaceNet (embeddings).\n",
    "  '''\n",
    "\n",
    "  # Convertendo a imagem para numpy e normalizando para o intervalo [0..1]\n",
    "  img = np.array(face).astype('float32')/255\n",
    "\n",
    "  # Expandindo a dimensão da imagem para adequá-la à entrada da FaceNet\n",
    "  # O shape deve ficar (1, 160, 160)\n",
    "  input = np.expand_dims(img, axis=0)\n",
    "\n",
    "  # Fazendo a predição do modelo para extrair os embeddings da imagem\n",
    "  embedding = facenet_model.predict(input)[0]\n",
    "\n",
    "  return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(embedding, mlp_model):\n",
    "\n",
    "    '''\n",
    "        Esta função é responsável por realizar a predição do modelo de classificação a partir das características de entrada.\n",
    "        Parâmetros:\n",
    "            * embedding: vetor de características de uma face, representado por um array numpy com 128 valores;\n",
    "            * mlp_model: modelo treinado do classificador MLP.\n",
    "        Retorno: um variável label, que terá valor 0 (maskoff) ou 1 (maskon).\n",
    "    '''\n",
    "\n",
    "    # Expandindo dimensão do array para adequá-lo à entrada do classificador\n",
    "    # A dimensão deve ficar (1, 128)\n",
    "    embedding = np.expand_dims(embedding, axis=0)\n",
    "\n",
    "    # Realizando a predição da probabilidade de acerto para cada classe\n",
    "    proba = mlp_model.predict_proba(embedding)\n",
    "\n",
    "    # Extraindo do vetor de probabilidades o label que apresentou o maior resultado\n",
    "    label = np.argmax(proba)\n",
    "\n",
    "    # Caso o modelo tenha menos de 95% de certeza sobre o maior resultado, o label predito receberá o valor inverso\n",
    "    # Por exemplo, se o modelo não tiver 95% ou mais de certeza sobre classificar como label 1, o retorno será label 0\n",
    "    if proba[0][label] < 0.95:\n",
    "        label = abs(label-1)\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_points_in_frame(frame):\n",
    "\n",
    "    '''\n",
    "        Este método é responsável por receber um frame (imagem), detectar as faces existentes, classificá-las de acordo com o uso da máscara e marcar sua categorização visivelmente através da OpenCV.\n",
    "        Parâmetro:\n",
    "            * frame: uma imagem de qualquer dimensão, pode ser um frame de um vídeo.\n",
    "        Retorno: uma imagem com marcações feitas através da OpenCV.\n",
    "    '''\n",
    "\n",
    "    # Transformando a imagem em array numpy\n",
    "    img = np.asarray(frame)\n",
    "\n",
    "    # Extraindo faces da imagem\n",
    "    faces = get_faces(img)\n",
    "\n",
    "    # Iniciando contador responsável por marcar quantas pessoas sem máscara existem na imagem\n",
    "    no_mask_count = 0\n",
    "\n",
    "    # Percorrendo a lista de faces identificadas\n",
    "    for face in faces:\n",
    "\n",
    "        # Extraindo os pontos da face\n",
    "        x1 = face['x1']\n",
    "        x2 = face['x2']\n",
    "        y1 = face['y1']\n",
    "        y2 = face['y2']\n",
    "\n",
    "        # Extraindo os embeddings da face\n",
    "        emb = get_embeddings(face['face'], facenet_model)\n",
    "\n",
    "        # Classificando a face em label 0 (maskon) ou label 1 (maskoff)\n",
    "        label = get_label(emb, mlp_model)\n",
    "\n",
    "        # Caso uma pessoa sem máscara seja identificada, o contador é incrementado\n",
    "        if not label: no_mask_count += 1\n",
    "\n",
    "        # Configurando o tipo e o tamanho da fonte para iniciar as marcações na imagem\n",
    "        font = cv.FONT_HERSHEY_TRIPLEX\n",
    "        font_scale = 0.5\n",
    "\n",
    "        # Desenhando um retângulo em torno da face\n",
    "        img = cv.rectangle(img, (x1, y1), (x2, y2), color[label], 2)\n",
    "\n",
    "        # Escrevendo a classificação MASK ou NO MASK em cima do retângulo desenhado\n",
    "        # Essa descrição é baseada no label\n",
    "        cv.putText(img, label_description[label], (x1, y1-10), font, fontScale=font_scale,\n",
    "                    color=color[label], thickness=1)\n",
    "\n",
    "        # Escrevendo no topo do frame um informativo sobre quantas pessoas estão sem máscara na imagem\n",
    "        cv.putText(img, f'People without mask: {no_mask_count}', (15, 15), font, fontScale=0.6,\n",
    "                    color=(0, 0, 0), thickness=1)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detectando o uso de máscara em um vídeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Iniciando captura...\n\nCaptura iniciada!\n"
    }
   ],
   "source": [
    "print('Iniciando captura...\\n')\n",
    "\n",
    "# Instanciando um objeto VideoCapture, para selecionar sua webcam\n",
    "# Também é possível renderizar um vídeo pronto, basta você passar o caminho desse arquivo no lugar do parâmetro 0\n",
    "vid = cv.VideoCapture(0)\n",
    "\n",
    "print('Captura iniciada!')\n",
    "\n",
    "# A captura dos frames através da sua webcam ou vídeo será feita até que você pressione a tecla 'q'\n",
    "while (True):\n",
    "\n",
    "    # Lendo a imagem e extraindo o frame\n",
    "    # O método read() retorna dois resultados. O primeiro diz se a captura do frame foi feita com sucesso ou não, enquanto o segundo entrega o frame capturado.\n",
    "    _, frame = vid.read()\n",
    "\n",
    "    # Fazendo a detecção de máscara e as marcações nas faces identificadas no frame\n",
    "    detec = mark_points_in_frame(frame)\n",
    "\n",
    "    # Exibindo o frame resultante do método anterior\n",
    "    cv.imshow('frame', detec)\n",
    "\n",
    "    # Condição de parada do loop: pressione a tecla 'q'\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Fechando o arquivo de vídeo ou dispositivo de captura\n",
    "vid.release()\n",
    "\n",
    "# Fechando as janelas abertas\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "# Apagando o objeto da memória\n",
    "del (vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Iniciando captura...\n\nCaptura iniciada!\n"
    }
   ],
   "source": [
    "print('Iniciando captura...\\n')\n",
    "\n",
    "# Instanciando um objeto VideoCapture, para selecionar sua webcam\n",
    "# Também é possível renderizar um vídeo pronto, basta você passar o caminho desse arquivo no lugar do parâmetro 0\n",
    "vid = cv.VideoCapture(0)\n",
    "\n",
    "print('Captura iniciada!')\n",
    "\n",
    "# A captura dos frames através da sua webcam ou vídeo será feita até que você pressione a tecla 'q'\n",
    "while (True):\n",
    "\n",
    "    # Lendo a imagem e extraindo o frame\n",
    "    # O método read() retorna dois resultados. O primeiro diz se a captura do frame foi feita com sucesso ou não, enquanto o segundo entrega o frame capturado.\n",
    "    _, frame = vid.read()\n",
    "\n",
    "    # Fazendo a detecção de máscara e as marcações nas faces identificadas no frame\n",
    "    # detec = mark_points_in_frame(frame)\n",
    "\n",
    "    # Exibindo o frame resultante do método anterior\n",
    "    cv.imshow('frame', frame)\n",
    "\n",
    "    # Condição de parada do loop: pressione a tecla 'q'\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Fechando o arquivo de vídeo ou dispositivo de captura\n",
    "vid.release()\n",
    "\n",
    "# Fechando as janelas abertas\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "# Apagando o objeto da memória\n",
    "del (vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bitfacerecognitionconda214a90b5a2cf478da9508f7114963a12",
   "display_name": "Python 3.7.9 64-bit ('face_recognition': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}